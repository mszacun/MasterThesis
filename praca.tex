\documentclass[a4paper, twoside, 12pt]{report}

\usepackage{titlesec}
\usepackage{polski}
\usepackage[english, polish]{babel}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{pdfpages}
\graphicspath{ {./images/} }
\usepackage[utf8]{inputenc}
\usepackage[margin=25mm]{geometry}
\usepackage{indentfirst}
\linespread{1}
\titleformat*{\section}{\fontsize{14pt}{2}\bfseries}
\titleformat*{\subsection}{\fontsize{13pt}{2}\bfseries}
\titleformat*{\subsubsection}{\fontsize{13pt}{2}\bfseries}
\usepackage[T1]{fontenc}
% \usepackage{tgtermes}

\newenvironment{abstractpage}
  {\vspace*{\fill}\thispagestyle{empty}}
    {\vfill}
    \renewenvironment{abstract}[1]
      {\bigskip\selectlanguage{#1}%
             \begin{center}\bfseries\abstractname\end{center}}
           {\par\bigskip}

\begin{document}

% \includepdf{stronatytulowa.pdf}


\begin{abstractpage}
\begin{abstract}{polish}
    W dzisiejszych czasach Internet pełen jest informacji na praktycznie dowolny temat. Wraz ze wzrastającą ilością
    dostępnych informacji, niezmiernie ważne staje się opracowanie mechanizmów pozwalających znaleźć informacje
    interesujące użytkownika. Nie jest to proste, ze względu na to, że informacje te zapisane są najczęściej w formie
    nieustrukturyzowanego tesktu. Tradycyjnie w tym celu wykorzystywane były wyszukiwarki internetowe. Niestety
    języki zapytań używane przez wyszukiwarki internetowe powodują, że wyrażenie precyzyjnej potrzeby informacyjnej
    przez użytkownika jest trudne, a niekiedy niemożliwe. Jednym z proponowanych rozwiązań tego problemu są systemy
    odpowiadające na pytania, w których zapytania formułowane są w formie pytań w języku naturalnym. Niniejsza
    praca zawiera krótkie przedstawienie dziedziny Question Answering, zajmującej się tworzeniem tego typu systemów,
    wraz z opisem systemu ,,Borsuk'' stworzonego na Politechnice Wrocławskiej.
    W głównej części pracy opisana została propozycja nowego mechanizmu ekstrakcji odpowiedzi z fragmentów dokumentów,
    opartego na zmodyfikowanej odległości edycyjnej jako miary podobieństwa między pytaniem a zdaniami mogącymi zawierać
    odpowiedź. Modyfikacja polega na dynamicznym przypisywaniu kosztu operacjom edycji w zależność od słowa. Przeprowadzone
    zostały badania pozwalające optymalnie dobrać wartości tych kosztów. Po zaimplementowaniu tego mechanizmu w systemie "Borsuk",
    wykonane zostały też badania mające na celu porównanie precyzji i dokładności odpowiadania na pytania przed i po
    zastosowaniu opisywanego mechanizmu.
\end{abstract}

\begin{abstract}{english}
    Nowadays the Internet is full of information on almost every topic. With growing amount of available information,
    providing ways to find specific information that user wants is becoming more and more important. Traditionally
    search engines were intended for that purpose. Unfortunately query languages used by search engines are sometimes
    insufficient for expressing user's infomation needs. One of proposed solutions are question answering systems, in which
    queries are formulated using natural language. This thesis contains quick overview of question answering discipline,
    which is concerned with build such systems and description of "Borsuk" system, created by Wrocław University of Technology.
    In main part, new answer extraction mechanism is presented. It is based on using modified tree edit distance as a measure of
    similarity question and sentences that ma contain answer. Modification lies in dynamically assing cost to edit operations
    based on word. Research was conducted to find out optimal cost for operations. Furthermore after implementing this
    mechanism in "Borsuk" question answering system, additional research was conducted to compare precision and accuracy
    of question answering before and after using described mechanism
\end{abstract}
\end{abstractpage}

\tableofcontents

\listoffigures

\chapter{Cel i~zakres pracy}
    Celem niniejszej pracy jest przedstawienie mechanizmu rozszerzającego system odpowiadający na pytania ,,Borsuk''
    pod względem dokładności udzielanych odpowiedzi. Zastosowany mechanizm wykorzystuje odległość edycyjną jako miarę
    podobieństwa między pytaniem a zdaniami zawierającymi odpowiedzi. W~ramach tego celu zostaną wykonane:
    \begin{itemize}
        \item prezentacja i ogólna charakterystyka dziedziny Question Answering wraz z przedstawieniem podstawowych
            pojęć,
        \item opis działania systemów odpowiadających na pytania, wraz z wyróżnieniem poszczególnych faz przetwarznia
            pytania i znajdowania odpowiedzi,
        \item przedstawienie przykładowych, innych niż zaproponowany w~pracy,
            sposobów ekstrakcji odpowiedzi na pytania z fragmentów dokumentów,
        \item zaprezentowanie mechanizmu ekstrakcji odpowiedzi z wykorzystaniem odległości edycyjnej między grafami
            reprezentującymi pytanie oraz zdania kandydujące na odpowiedzi,
        \item przeprowadzenie badań pozwalających na dobranie optymalnych parametrów działania opisywanego
            mechanizmu ekstrakcji odpowiedzi wraz z analizą wniosków z nich płynących,
       \item zaprezentowanie sprawozdania z przebiegu oraz wyników badań porównujących precyzję i dokładność
           odpowiadania na pytania przez system "Borsuk" przed i po implementacji opisanego mechanizmu ekstrakcji
           odpowiedzi.
       \item przeanalizowanie możliwych sposobów kontynuacji prac nad udoskonaleniem przedstawionego mechanizmu

    \end{itemize}

\chapter{Wstęp}
    \section{Przetwarzanie języka naturalnego}
        \subsection{Podstawowe pojęcia}
            \emph{Przetwarzanie języka naturalnego} jest to dziedzina informatyki korzystającą z osiągnięć
            między innymi: sztucznej inteligencji, uczenia maszynowego, nauk kognitywnych
            (zajmujących się obserwacją i~analizą działania ludzkich zmysłów), lingwistyki komputerowej oraz
            wielu innych. Głównym obszarem zainteresowań tej dziedziny jest budowa systemów oraz narzędzi
            umożliwiających wykonywanie użytecznych zadań wiążących się z wykorzystaniem języka naturalnego.
            Obejmuje to zadania takie jak: komunikacja człowiek-komputer, wspomaganie komunikacji międzyludzkiej,
            czy też przetwarzanie tekstu w języku naturalnym bądź mowy\cite{SPEECHANDLANGUAGEPROCESSING}.

            \emph{Język naturalny} jest to język,
            którym posługują się ludzie w codziennej komunikacji między sobą, w sposób naturalny (na przykład
            język polski). Jest to cecha odróżniająca języki naturalne od \emph{języków sztucznych} stworzonych
            do specyficznych zastosowań (na przykład język matematyczny), w tym do komunikacji z komputerem
            (na przykład języki programowania: Assembler, C++, Python).

            Aby umożliwić tworzenie nowych technik
            przetwarzania języka naturalnego oraz ich testowanie niezbędny jest reprezentatywny i zbalansowany
            zbiór tekstów w danym języku. Powinien on zawierać teksty na tyle zróżnicowane, aby pokazać zmienność
            i różne zastosowania języka naturalnego. Przygotowanie takiego zbioru nie wystarczy jednak, aby
            komputer zaczął rozumieć język naturalny. Do tekstów należy dodać metadane, które pozwolą komputerowi
            łatwiej znaleźć wzorce i zależność występujące w danym języku. Jako przykład takich metadanych można
            wymienić: oznaczenie części mowy oraz ich form gramatycznych czy też oznaczenie bytów nazwanych.
            Zbiór danych przygotowany w ten sposób nazywane jest \emph{korpusem}\cite{NATURALLANGUGEANNOTATION}
            i może zostać wykorzystany w różnego rodzaju algorytmach uczenia maszynowego.
        % \subsection {Główne problemy}
        % Ambigoity - słów, części mowy, wyrażanie jednej myśli na wiele sposbów
        % Implicit information, informacje zapisane nie wprost, zaimki, podmioty domyślne itp.
        \subsection {Zastosowanie}
            Przetwarzanie języka naturalnego nie jest nową dziedziną informatyki. Pierwsze prace nad tą technologią
            rozpoczęły się we wczesnych latach 50. XX wieku. Podobnie jak w przypadku innych dziedzin informatyki,
            największym zainteresowanym rozwojem przetwarzania języka naturalnego i sponsorem badań był Deparatament
            Obrony Stanów Zjednoczonych. Głównym celem nad którym skupiały się badania było automatyczne
            \emph{tłumaczenie tekstów} z jednego języka na drugi, w szczególności z rosyjskiego na
            angielski\cite{NLPHISTORY}. Od tego czasu w dziedzinie maszynowego tłumaczenia został poczyniony
            ogromny postęp. Początkowo systemy te wykorzystywały proste podstawienia zamieniające słowa z jednego
            języka na słowa z drugiego. Dodatkowo były one wzbogacane o reguły semantyczne określające w jaki
            sposób przetłumaczyć określone fragmenty tekstu.  Obecnie w automatycznym tłumaczeniu wykorzystywane
            są elementy uczenia maszynowego z użyciem opisanych wcześniej korpusów językowych oraz sieci neuronowe.
            Oprogramowanie zbudowane w ten sposób nie jest w stanie co prawda przetłumaczyć dzieł Szekspira,
            zachowując ich artystyczną i estetyczną wartość. Znakomicie radzi sobie jednak, jak na razie jeszcze
            pod kontrolą człowieka, z tłumaczeniem standardowych tekstów tworzonych przez ludzi na przykład
            sprawozdań biznesowych\cite{INTROTOMACHINETRANSLATION}. Wykorzystanie tego typu systemów pozwala skrócić i
            ułatwić pracę tłumacza, co w konsekwencji przyczynia się do zmniejszenia kosztów tłumaczeń.

            Innym przykładem zastosowania przetwarzania języka naturalnego jest \emph{automatyczne streszczenie tekstów}.
            Jest to proces skracania zazwyczaj długiego tekstu w języku naturalnym do formy krótkiego streszczenia,
            zawierającego jedynie najważniejsze informacje w nim zawarte. Tego typu streszczenia pozwalają
            użytkownikowi szybko ocenić, czy dany dokument ma dla niego znaczenie, bez konieczności czytania go całego.
            Jest to szczególnie istotne na przykład w przypadku dokumentów zwracanych przez wyszukiwarki internetowe.
            System tworzące streszczenia najczęściej oprócz samego tekstu otrzymują także parametr
            mówiący o maksymalnej ilość słów w streszczeniu. Istnieją dwa podejścia do problemu generowania streszczeń.
            Pierwsze z nich polega na wybraniu najważniejszych zdań, bądź fragmentów z oryginalnego tekstu. Wybierane
            są zdania, zawierające słowa często występujące w dokumencie, ponieważ takie słowa mogą mieć duże znaczenie
            w kontekście tematu dokumentu i w związku z tym warto umieścić zdania je zawierające w podsumowaniu. Miejsce,
            w którym znajduje się zdanie również może wpłynąć na to, czy zdanie znajdzie się w streszczeniu. Szczególne
            znaczenie mają tutaj zdania zawarte w tytule dokumentu oraz podtytułach poszczególnych akapitów. Należy
            także zwrócić uwagę na zdania zawierające typowe zwroty używane do podsumowania bądź wprowadzenia do tematu,
            takie jak między innymi: ,,Podsumowując...'', ,,Artykuł zawiera'' i wiele innych.

            Alternatywnym podejściem do tworzenia streszczeń dokumentów jest zbudowanie wewnętrznej reprezentacji
            dokumentu, znalezienie najistotniejszych informacji w nim zawartych, a następnie wykorzystanie metod
            generowania języka naturalnego do stworzenia skrótu. Tego typu podejście najczęściej wykorzystywane jest
            do tworzenia bardzo krótkich streszczeń, na przykład nagłówków składających się jedynie z jednego zdania.
            Stosowane są tutaj różnorakie wzorce, które wypełniane są informacjami uzyskanymi z dokumentu. Do zbudowania
            reprezentacji dokumentu najczęściej wykorzystywane są metody głębokiej analizy języka naturalnego,
            wykorzystujące między innymi reprezentację semantyczną. W porównaniu z ekstrakcją zdań z dokumentu, generowanie
            streszczeń jest techniką o wiele rzadziej wykorzystywaną i słabiej zbadaną\cite{SUMMARIZATIONOVERVIEW}.

            Obecnie większość edytorów tekstowych, programów pocztowych, czy też przeglądarek internetowych zawiera
            funkcję automatycznego poprawiania błędów ortograficznych
            oraz tak zwanych "literówek". Funkcja ta jest implementowana przy pomocy \emph{korektora pisowni}. Jego
            zadaniem jest wskazywanie fragmentów tekstu, w których znajdują się błędy pisowni. Tego typu oprogramowanie
            najczęściej oparte jest o wcześniej przygotowane słowniki poprawnie wprowadzonych słów. Następnie każde
            słowo tekstu jest wyszukiwane w słowniku i w przypadku jego braku, sygnalizowany jest błąd pisowni. Oprócz
            samej listy słów, w słowniku mogą znajdować się dodatkowe informacje takie jak na przykład punkty podziału
            słów (przydatne przy automatycznym formatowaniu tekstu i przenoszenia słów do nowej linii). Ponadto korektor
            pisowni może zawierać algorytmy, dedykowane dla danego języka, radzące sobie z odmianą słów w różnych
            kontekstach (poprawność fleksyjna). Do wskazania sugestii dotyczących autokorekty wykorzystywana jest miara
            odległości edycyjnej bądź odległości Hamminga. Odmiennym podejściem do problemu poprawiania pisowni jest zastosowanie
            metod statystycznych. Do rozpoznania błędów najczęściej wykorzystywane są \emph{n-gramy}, ich wykorzystanie
            wymaga jednak przygotowanie dużej ilości danych uczących, takich jak omawiany wcześniej korpus językowy
            \cite{SPELLCHECKING}.

            Kolejnym przykładem zastosowania przetwarzania języka naturalnego jest \emph{ekstrakcja informacji}. Zadanie
            to polega na wydobyciu informacji z nieustrukturyzowanego tekstu w języku naturalnym i zapisania ich w
            formacie o określonej strukturze,w celu dalszego
            przetwarzania. Polegać to może na przykład na tworzeniu rekordów medycznych w bazie danych na podstawie
            dokumentacji medycznej wypełnionej przez lekarza w języku polskim. Ekstrakcja informacji jest zadaniem
            trudnym, dlatego oprogramowanie tego typu najczęściej jest opracowywane pod kątem konkretnej dziedziny i
            dostosowywane do niej. Jak zostało wcześniej wspomniane, większość zasobów informacji zapisana jest w
            formie nieustrukturyzowanego tekstu w języku naturalnym, w związku z czym ich wyszukiwanie nie jest łatwe.
            Wraz ze wzrastającą ilością tych informacji, rośnie znaczenie problemu ich ekstrakcji. Dzięki
            przekształceniu tych informacji do postaci rekordów w bazie danych wyszukiwanie staje się o wiele prostsze.
            Zazwyczaj z tekstu wydobywane są byty nazwane, relacje między nimi oraz wydarzenia, w których te byty
            biorą udział, na przykład: ,,W marcu zeszłego roku pacjent Jan Nowak był operowany w szpitalu w Leśnej
            Górze''. Obecnie dynamicznie rozwija się dziedzina ekstrakcji informacji z wielu źródeł w przeciwieństwie
            do jednego dokumentu. W przeszłości wydobywanie informacji odbywało się w oparciu o ręcznie tworzone
            reguły i wzorce lingwistyczne. Obecnie wzorce i reguły są najczęściej wyuczane w sposób automatyczny,
            a same wzorce stały się bardziej rozmyte dzięki zastosowaniu \emph{ukrytych modeli Markova (HMM)} oraz
            \emph{warunkowych pól losowych (CRF)}\cite{INFORMATIONEXTRACTION}.

            Jednym z najbardziej znanych zastosowań przetwarzania języka naturalnego jest tworzenie programów
            prowadzących z użytkownikiem dialog w języku naturalnym, tak zwanych \emph{programów konwersujących}.
            Ten dział inżynierii języka naturalnego rozgłos uzyskał dzięki testowi zaproponowanemu przez Alana Turinga,
            który miał udowodnić posiadanie przez maszynę inteligencji podobnej do ludzkiej. W podobny sposób człowiek
            określa inteligencję innych ludzi, również na podstawie tego, co i jak mówią. Test ten polega na swobodnej
            rozmowie ,,sędziego'' z pozostałymi uczestnikami testu, wśród których znajduje się maszyna. Jeśli sędzia
            nie jest w stanie określić, który z uczestników jest maszyną, zakłada się, że przeszła ona test.
            Programy konwersujące są nie tylko ważnym punktem zainteresowań badań nad sztuczną inteligencją, wiele z
            nich powstaje w celu czysto rozrywkowym. Oprócz tego, wiele firm wykorzystuje tego typu aplikacje,
            jako wirtualnych asystentów na swoich stronach internetowych. Mogą oni na przykład przyjąć zamówienie od
            klienta, opowiedzieć o funkcjonowaniu i działalności firmy, oraz odpowiedzieć na wiele pytań użytkownika.
            Obecnie bardzo popularne stają się aplikacje typu ,,osobisty asystent'', w których moduł konwersujący jest
            kluczowym elementem. Aplikacje te dostępne są w praktycznie każdym smartphonie. Ze względu na wysoki
            poziom skomplikowania tej dziedziny przetwarzania języka, wykorzystuje ona praktycznie wszystkie z
            przedstawionych wcześniej technik, od powierzchownej i głębokiej analizy tekstu, poprzez wzorce i reguły
            lingwistyczne, kończąc na sieciach neuronowych i maszynowym uczeniu

            Zdolność zrozumienia emocji odczuwanych przez drugą osobę pozwala ma skuteczniejszą komunikację oraz
            umożliwia łatwiejsze rozpoznanie oczekiwań i potrzeb innych ludzi. Firmy są w stanie zainwestować duże
            pieniądze w badanie rynku, aby rozpoznać nastawienie oraz opinie ludzi na temat ich produktów. Dzięki
            tej wiedzy mogą oni je udoskonalać, w taki sposób aby lepiej zaspokajały zapotrzebowanie klientów.
            Liczba opinii na temat produktów znajdujących się w Internecie jest ogromna i nie możliwa do
            przestudiowania przez człowieka. W związku z tym pojawia się zapotrzebowanie na oprogramowanie pozwalające
            zautomatyzować proces \emph{analizy wydźwięku} w tekście pisanym. Tradycyjnie oprogramowanie
            przeprowadzające ten proces opierało się na wyszukiwaniu słów pozwalających jednoznacznie określić
            nastawienie autora, na przykład ,,wesoły'', ,,smutny'', ,,dobre'', ,,złe'' i tym podobne. W dzisiejszych
            czasach wykorzystywane są różnego rodzaju metody statystyczne oraz uczenie maszynowe, a zwłaszcza rozmaite
            klasyfikatory\cite{SENTIMENTANALYSIS}.

        \subsection{Wykorzystywane narzędzia}
            Przetwarzanie języka naturalnego składa się zazwyczaj z następujących po sobie etapów, połączonych w łańcuch,
            gdzie wyjście jednego z etapów jest wejściem poprzedniego. Poszczególne etapy zostały opisane poniżej.

            Tekst, który należy przedstawić może występować w postaci dokumentów o różnych długościach, poczynając od
            krótkich wiadomości tekstowych, kończąc na wielotomowych powieściach. Pierwszym etapem przetwarzania języka
            naturalnego jest podział tekstu na mniejsze fragmenty, czyli tak zwana \emph{segmentacja}. Segmentacja jest
            kluczową częścią każdego systemu przetwarzającego język naturalny, ponieważ poprawnie wyodrębnione części
            dokumentu są podstawowymi jednostkami wykorzystywanymi na dalszych etapach przetwarzania języka.
            Do przeprowadzenia segmentacji wykorzystywane są narzędzia zwane
            \emph{tokenizerami}.% sprawdzic polska nazwe, mozna zasugerowac sie MACCA
            W zależności od zastosowania możliwy jest podział na różne jednostki, między innymi wyrazy, zdania, czy też akapity.
            Może się wydawać, że jest to łatwe zadanie, jednak pojawiają się problemy z na przykład: wyrazami składającymi
            się z kilku części (np. Bielsko Biała), partykułą ,,się'' (czy ,,bawić się'' potraktować jako jedno słowo, czy dwa?)
            czy też z wyrażeniami wielowyrazowymi, na przykład nazwami własnymi, które często składają się z kilku wyrazów.
            Przy podziale tekstu na zdania należy zauważyć, że nie każda kropka występująca w tekście kończy zdanie, oprócz
            tego kropkę stawia się na końcu niektórych skrótów, mogą być też wykorzystywane na przykład do zapisu daty.
            Opracowanie poprawnych tokenizerów jest bardzo ważne, ponieważ wszystkie błędy popełnione w czasie segmentacji,
            propagują się do następnych etapów przetwarzania języka. W procesie segmentacji najczęściej wykorzystuje się
            zasady zapisane w postaci gramatyki regularnej automatu skończonego, bądź transduktora. Tokenizery przygotowane
            w ten sposób są w stanie osiągnąć dużą dokładność, jednak są one najczęściej dostosowane do specyficznego języka
            naturalnego. Wykorzystanie ich do innego języka jest bardzo trudne i najczęściej wymaga stworzenia nowego
            zestawu reguł. Obecnie coraz częściej stosowane są metody, które można trenować, wykorzystujące uczenie maszynowe,
            drzewa decyzyjne, bądź też metody statystyczne oparte na modelowaniu maksimum entropii\cite{HANDBOOKNLP}.

            Kolejnym etapem przetwarzania języka naturalnego jest \emph{analiza fleksyjna}. Na tym etapie odbywają się dwa
            ważne procesy: \emph{steminga} i \emph{analiza morfologiczna}. Steming polega na sprowadzeniu różnych form fleksyjnych wyrazu do jednej
            reprezentacyjnej postaci. Lematyzacja jest odmianą stemingu polegająca na sprowadzeniu form wyrazowych do lematu.
            Lemat jest to forma wyrazu pod jaką szukalibyśmy go w na przykład w słowniku, najczęściej najprostsza
            (na przykład dla czasowników: bezokolicznik). Steming jest najczęściej przeprowadzany w oparciu o wcześniej
            przygotowane słowniki bądź reguły.

            Alternatywą dla stemingu jest \emph{analiza morfologiczna}, która stara się otagować każdy wyraz poprzez odgadnięcie
            jego części mowy oraz formy fleksyjnej. Każdy wyraz zostaje przypisany do jednej z wcześniej określonych
            przez lingwistów kategorii gramatycznych pochodzących z tak zwanych \emph{tagsetów}. Aplikacje zajmujące
            się przypisywaniem tagów do wyrazów nazywamy \emph{taggerami części mowy (part-of-speech tagger, POS}. Dzięki przypisaniu
            wyrazom kategorii gramatycznych, można uzyskać wiele informacji na temat tych wyrazów jak i ich sąsiedztwa,
            co jest bardzo ważne na dalszych etapach przetwarzania. Częstym problemem jest wieloznaczność wyrazów, na przykład
            wyraz ,,mam'' może oznaczać, że ja coś posiadam, bądź też może być biernikiem liczby mnogiej wyrazu ,,mama''.
            Taggery można pogrupować w dwie kategorie: taggery oparte o zbiór zasad oraz taggery stochastyczne. Zasady
            dla taggerów mogą być definiowane ręcznie, bądź nauczone maszynowo. Dodatkowo dodawane są reguły dla wyrazów
            niejednoznacznych. Taggery stochastyczne używają wcześniej przygotowanego i oznaczonego korpusu do zbudowania
            modelu probabilistycznego, na przykład Ukrytego Modelu Markova, który określa prawdopodobieństwo, że dany
            wyraz będzie miał dany tag w określonym kontekście\cite{SPEECHANDLANGUAGEPROCESSING}.

            Po analizie fleksyjnej następuje \emph{parsowanie syntaktyczne}. Jego celem jest przeanalizowanie ciągu wyrazów
            (zazwyczaj zdania) w celu zidentyfikowania jego struktury w oparciu o gramatykę formalną. Dzięki temu procesowi
            w następnych etapach przetwarzania możliwe jest przypisanie znaczenia do całego zdania. W tym celu na podstawie
            gramatyki tworzona jest hierarchiczna, syntaktyczna struktura, która nadaje się do semantycznej interpretacji.
            Dzięki parsowaniu syntaktycznym możliwe jest wykrycie, że grupa słów może być frazą, na przykład frazą
            rzeczownikową: ,,piękna dziewczyna''.
            Gramatyka formalna składa się ze zbioru reguł, tak zwanych produkcji, z których każda mówi w jaki sposób symbole występujące
            w języku mogą być grupowane razem, oraz ze zbioru symboli terminalnych, które mówią o symbolach występujących
            w języku.
            Gramatyki formalne z powodzeniem wykorzystywane są do parsowania języków sztucznych (języki programowania),
            jednak różnią się one w stosunku do gramatyk wykorzystywanych do parsowania języka naturalnego. Jak zostało
            wcześniej wspomniane, język naturalny charakteryzuje się niejednoznacznością, również na poziomie syntaktycznym.
            % TODO: wymyslic to zdanie
            Przykładowo zdanie ,,'' może zostać zinterpretowane na dwa sposoby.
            Gramatyki obsługujące języki sztuczne są zazwyczaj kompletne, a ponadto zakładają, że przekazany tekst
            (najczęściej kod programu) będzie poprawny, jeśli tak nie jest, użytkownikowi zgłaszany jest błąd i zmuszony
            jest on do poprawy kodu. W przypadku języka naturalnego, gramatyka ze względu na bogactwo językowe i ciągły
            rozwój języka, nigdy nie jest kompletna. Ponadto w tekstach dokumentów często zdarzają się błędy językowe,
            które oprogramowanie powinno być w stanie obsłużyć. Podczas przetwarzania języka naturalnego ciężko jest odróżnić,
            czy błąd parsowania nastąpił na skutek nieprawidłowych danych, czy też ze względu na niekompletność gramatyki.

            Po parsowaniu syntaktycznym w razie potrzeby może zostać przeprowadzone \emph{rozpoznawanie nazw własnych}. Proces
            ten polega na znalezieniu i oznaczeniu w tekście bytów nazwanych oraz przypisaniu ich do jednej z wcześniej
            zdefiniowanych kategorii. Oprogramowanie tego typu nazywane jest z angielskiego \emph{named entity recognizer (NER)}.
            Wśród najczęściej wykorzystywanych kategorii bytów nazwanych wyróżniamy kategorie
            takie jak: osoba, organizacja, miejsce, wyrażenie czasowe czy też wyrażenia liczbowe (na przykład kwoty, procenty).
            Podobnie jak w czasie analizy morfologicznej, tak i tutaj wyrazowi, bądź grupie wyrazów zostaje przypisany
            tag, mówiący o tym do jakiej kategorii on należy. Rozpoznawanie oparte jest o reguły rozpoznające i klasyfikujące
            aktywowane przez cechy specyficzne dla pozytywnych i negatywnych przykładów nazw własnych. Wiele wczesnych
            taggerów wykorzystywało najczęściej ręcznie przygotowane heurystyczne reguły. Nowoczesne systemy skupiają się
            raczej na wykorzystaniu algorytmów uczenia maszynowego do stworzenia reguł na podstawie wcześniej przygotowanych
            danych uczących pochodzących z oznaczonych korpusów. W przypadku braku danych treningowych, ręcznie przygotowane
            reguły są jedną dostępną metodą. Wśród wykorzystywanych do rozpoznawania nazw własnych metod uczenia maszynowego
            wyróżniamy: ukryte modele Markova (HMM), drzewa decyzjne, modele maksymalnej entropii (ME), maszyny wektorów nośnych (SVM)
            oraz warunkowe pola losowe (CRF). Wszystkie te metody są metodami uczenia nadzorowanego\cite{NERSURVEY}.

            Ostatnim narzędziem ogólnego przeznaczenia wykorzystywanym w przetwarzaniu języka naturalnego jest
            \emph{parser semantyczny}. Pozwala on zrealizować ostateczny cel przetwarzania języka naturalnego, którym
            jest jest zrozumienie wypowiedzi. Zrozumienie wypowiedzi może polegać na przyswojeniu nowej wiedzy w niej
            zawartej, bądź też na wykonaniu jakiejś akcji w odpowiedzi na wypowiedź. Generalnie analiza semantyczna
            polega na analizie znaczeń słów, wyrażeń, zdań i wypowiedzi biorąc pod uwagę kontekst w jakim występują.
            W praktyce wiąże się to z przetłumaczeniem oryginalnego tekstu w języku naturalnym do pewnego metajęzyka
            lub specjalnie zaprojektowanej semantycznej reprezentacji wiedzy. W stosunku do reprezentacji znaczenia
            wypowiedzi stawiane jest kilka wymagań. Po pierwsze reprezentacja musi umożliwiać porównanie znaczenia wypowiedzi
            z bazą wiedzy o świecie. Innymi słowy mówiąc, powinno być możliwe zweryfikowanie, czy dana wypowiedź jest
            prawdziwa czy fałszywa, w oparciu o posiadaną przez system wiedzę. Kolejnym problemem jest bogactwo języków
            naturalnych. Weźmy pod uwagę następujące zdania:
            \begin{enumerate}
                \item Czy biblioteka jest otwarta w sobotę?
                \item Czy można skorzystać z biblioteki w sobotę?
                \item Da się wypożyczyć książkę z biblioteki w sobotę?
            \end{enumerate}
            Ze względu na to, że wszystkie te zdania zbudowane są z różnych wyrazów, o różnych formach gramatycznych oraz
            interpretacji syntaktycznej, można by się spodziewać, że otrzymają one różne reprezentacje semantyczne. Znacznie
            utrudniłoby to proces weryfikacji prawdziwości tych wypowiedzi. W przypadku, gdy w bazie wiedzy systemu istniałby
            jedynie jedna reprezentacja faktu otwarcia biblioteki w sobobty, system zwerfyikowałby tylko jedno z tych zdań
            jako prawdziwe, to które pasuje do sposobu reprezentacji faktu w bazie wiedzy. Porządanym rozwiązaniem tego
            problemu jest przypisanie wszystkim tym zdaniom takiej samej reprezentacji semantycznej, ze względu na to, że
            odpowiedź na wszystkie te pytania jest taka sama. Reprezentacja semantyczna powinna pozwolić na wyrażenie
            znaczenie jak największej ilości wypowiedzi, w idealnym przypadku pojedynczy system reprezentacji pozwoliłby
            na na przedstawienie każdej wypowiedzi. Jak dotąd jednak taka reprezentacja nie została znaleziona, dlatego
            w zależności od zastosowania stosowane są różne systemy reprezentacji znaczenia. Każda z reprezentacji
            pozwala na przedstawienie obiektów, właściwości tych obiektów oraz zależności pomiędzy różnymi obiektami.
            Jedną z przykładowych reprezentacji jest logika pierwszego rzędu. Jest to elastyczna reprezentacja semantyczna,
            która może być w łatwy sposób przetwarzana przez systemy komputerowe. Stałe występujące występujące w języku
            logiki pozwalają na specyfikowanie obiektów z relanego świata, dzięki funkcjom możemy opisywać właściwości
            obiektów, oraz relacje pomiędzy obiektami, natomiast zmienne pozwalają na odnoszenie się do obiektów bez wskazywania
            konkretnego z nich. Pozwala to na tworzenie twierdzeń na temat konkretnego nieznanego obiektu, bądź na temat
            wszystkich obiektów z określonej dziedziny. Przykładowe wyrażenie w logice pierwszego rzędu zostały przedstawione
            poniżej:
            \begin{itemize}
                \item Biblioteka Główna jest otwarta w sobotę: $ Otwarta(BibliotekaGlowna, Sobota) $
                \item Która biblioteka jest otwarta w sobotę?: $ \exists b \in Biblioteki: Otwarta(b, Sobota) $
                \item Która biblioteka we Wrocławiu posiada książki na temat NLP?: \\ $ \exists b \in Biblioteki: Lokalizacja(b) = Wroclaw \land Posiada(b, KsiazkiNLP) $
            \end {itemize}


    \section{Odpowiadanie na pytania (Question Answering)}
        \subsection{Podstawowe pojęcia}
            \emph{System odpowiadający na pytania} można zdefiniować jako system, który jest w stanie automatycznie
            zrozumieć treść pytania zadanego w języku naturalnym, oraz w odpowiedzi dostarczyć żądane informacje\cite{HANDBOOKNLP}.

            Dziedzina przetwarzania języka naturalnego zajmująca się odpowiadaniem na pytania zrodziła się z dwóch powodów.
            Po pierwsze standardowe języki w których formułuje się zapytania do wyszukiwarek internetowych nie zawsze
            pozwalają na wyrażenie \emph{potrzeby informacyjnej} użytkownika w prosty i przyjazny sposób. Dotyczy to
            przede wszystkim przypadków gdy użytkownik potrzebuje bardzo szczegółowych informacji na przykład:
            ,,Jak nazywa się żeński odpowiednik El Nino, który powoduje obniżenie temperatury i bardzo suchą pogodę?''.
            Systemy odpowiadające na pytania potrafią wychwycić potrzebę informacyjną użytkownika użytkownika zawartą w
            pytaniu i wyręczają go w formułowaniu zapytań do wyszukiwarki. Może on
            wprowadzić swoje zapytanie w dokładnie taki sam sposób jakby pytał on drugiego człowieka.

            Drugi powód również związany jest z poszukiwaniem szczegółowych informacji przez użytkownika. Tradycyjne
            wyszukiwarki w odpowiedzi na zapytanie zwracają całe dokumenty, w których znajdują się słowa kluczowe podane
            przez użytkownika. Jest to do zaakceptowania, gdy poszukuje on ogólnych informacji na dany temat, jednak
            gdy interesuje go konkretna informacja, zmuszanie użytkownika do czytania długiego tekstu nie jest najlepszym
            pomysłem. Systemy odpowiadające na pytania posiadają mechanizmy wyodrębniania odpowiedzi z tekstu dokumentów,
            dzięki czemu, jako wynik prezentują użytkownikowi jedynie krótkie fragmenty, w których, według systemu,
            najprawdopodobniej znajduje się odpowiedź na jego pytanie.

            Najprostszym rodzajem pytań na które odpowiadają
            systemy i jednocześnie najszerzej zbadaną dziedziną są \emph{pytania o fakty}. Są to pytania o proste fakty,
            dla których odpowiedź może zostać bezpośrednio wydobyta z tekstów źródłowych. Przykładem tego typu pytań są
            pytania zadawane w teleturniejach typu ,,Jeden z dziesięciu'', ,,Milionerzy'', ,,Jeopardy!'' i tym podobne,
            w których odpowiedzią na pytanie jest byt nazwany, na przykład miejsce, osoba, organizacja i tak dalej.
            Odpowiadanie na pytania o fakty, jest w dużej mierze oparte o techniki ekstrakcji informacji, które opisane
            zostały wcześniej.
            Innym przykładem typu pytań są \emph{pytania o definicje}, w których użytkownik przedstawia systemowi pewne
            pojęcie i prosi o znalezienie jego definicji, na przykład ,,Co to jest Yerba Mate?''. W przeciwieństwie do
            pytań o fakty, odpowiedź na pytanie o definicje jest najczęściej potencjalnie długim tekstem przytaczającym
            najważniejsze fakty związane z obiektem zainteresowania użytkownika. Wśród innych typów
            pytań wyróżniamy pytania ,,Jak?'', ,,Czy?'', pytania hipotetyczne, pytania o porównanie dwóch obiektów oraz
            inne, które nie otrzymują tak dużo uwagi, jak pytania o fakty i definicje.

            Pierwsze z systemów odpowiadających na pytania, były ograniczone do odpowiadania na pytania pochodzące z
            określonej dziedziny, były to tak zwane \emph{Closed-domain question answering systems}. Przeciwieństwem
            tego typu systemów, są systemy ogólnego przeznaczenia, które są w stanie odpowiedzieć na pytania z dowolnej
            dziedziny, systemy te nazywane są \emph{Open-domain question answering systems}. Odpowiadanie
            na pytania z pewnej ograniczonej dziedziny jest łatwiejsze, ponieważ możliwe jest wykorzystanie specyficznej
            wiedzy dziedzinowej. Co więcej liczba różnych rodzajów pytań z jakimi musi mierzyć się system odpowiadający
            jest najczęściej znacznie mniejsza, gdy pytania dotyczą tylko jednej dziedziny. Niniejsza praca skupia się
            na systemie odpowiadającym na pytania o fakty niezależnie od dziedziny.

        \subsection{Historia}
            Początek badań związanych z odpowiadaniem na pytania datowany jest na rok 1959. Jedną z pierwszych prac
            naukowych na w tej dziedzinie, była praca z 1965 roku\cite{FIRSTWORKQA}, w której opisane zostało piętnaście
            istniejących w tamtym okresie systemów związanych z odpowiadaniem na pytania. Mimo zastosowania różnorodnych
            podejść do problemu, autor zastrzega, że żaden z tych systemów nie mógłby zostać zastosowany w praktyce,
            jednak napawają one nadzieją na powstanie takiego systemu w przyszłości.

            Cechą wspólną pierwszych systemów tego typu było wykorzystanie wiedzy zapisanej w postaci ustrukturyzowanych
            baz danych jako źródło odpowiedzi. Oznaczało to, że mogły one odpowiedzieć jedynie na pytania z określonej
            dziedziny, z której informacje zostały wcześniej zapisane w bazie danych. Jednymi z najbardziej systemów
            w tym czasie były \emph{system LUNAR}\cite{LUNAR}, który odpowiadał na pytania na temat danych geologicznych
            zebranych na Księżycu oraz \emph{system BASEBALL}\cite{BASEBALL}, który posiadał informacje na temat meczy rozegranych w
            amerykańskiej lidze baseballu (MLA) w czasie jednego sezonu. Oba te systemy analizowały pytanie wprowadzone
            przez użytkownika, a następnie na jego podstawie budowały zapytanie do ustrukturyzowanej bazy danych. Można
            powiedzieć, że systemy te były po prostu interfejsem pozwalającym na dostęp do bazy danych za pomocą języka
            naturalnego ludziom bez wiedzy z dziedziny baz danych. Oprócz wymienionych systemów, powstało jeszcze wiele
            innych, każdy dostosowany do bazy danych zawierającej informacje z innej dziedziny. Ograniczenie do jednej
            dziedziny było największą wadą tych systemów, jednak miały one duży udział w rozwój syntaktycznej i
            semantycznej analizy pytań użytkownika\cite{QASURVEY}.

            Znaczny rozwój systemów odpowiadających na pytania niezależne od dziedziny na podstawie dużych kolekcji
            tekstów rozpoczął się wraz z pojawieniem się tej dziedziny na konferencji \emph{TREC (Text REtrieval Conference)}
            w 1999 roku.
            Wzrosło wtedy zainteresowanie systemami, które mogłyby zostać praktycznie wykorzystane na dużą skalę. Dzięki
            dodaniu QA do konferencji TREC, opracowane zostały metody oraz kryteria ewaluacji systemów odpowiadających
            na pytania. Na konferencji testowano różne metody i implementacje odpowiadania na pytania w ujednolicony i
            ustandaryzowany sposób. Umożliwiło to porównywanie systemów między sobą w jednoznaczny sposób oraz pozwoliło na ocenę
            czy rozwój tych systemów poprawia uzyskiwane przez nie wyniki. Dzięki konferencji badacze prowadzący badania
            w dziedzinie QA mogli swobodnie prezentować wyniki swoich badań, wymieniać między sobą pomysły oraz pracować
            nad kolejnymi, udoskonalonymi, systemami.

            Obecnie najnowocześniejsze systemy odpowiadające na pytania są w stanie osiągnąć bardzo dobre wyniki jeśli
            chodzi o precyzję i dokładność odpowiedzi na pytania użytkowników. Jest to możliwe dzięki jednoczesnemu zastosowaniu
            wielu różnych metod analizy pytań użytkownika oraz ekstrakcji odpowiedzi, a następnie przeprowadzenie efektywnego
            rankingu odpowiedzi w celu wybrania tej najlepszej.

            Jednym z szerzej znanych nowoczesnych systemów QA jest Watson stworzony przez naukowców z firm IBM.
            Został on specjalnie zaprojektowany, działa zarówno na dedykowanym sprzęcie jak i oprogramowaniu stworzonym
            przez ekspertów z IBM i jest zoptymalizowany pod względem przetwarzania równoległego.
            Według twórców Watson wykorzystuje ponad 100 technik analizowania języka naturalnego, identyfikowania źródeł,
            generowania hipotez, znajdowania i oceniania dowodów dla hipotez oraz ich łączenia\cite{WATSON}.
            Pokazem możliwości Watsona był jego udział w amerykańskim teleturnieju ,,Jeopardy!''. W programie tym
            trzech uczestników rywalizuje ze sobą poprzez odpowiadania na pytania zadawane w języku naturalnym dotyczące
            szerokiej gamy tematów, włączając w to historię, wydarzenia aktualne, naukę, sztukę oraz kulturę masową.
            Pytania te często są skomplikowane językowo, zawierają niejednoznaczne określenia, związki frazeologiczne,
            żartobliwą grę słów oraz różnego rodzaju nawiązania.
            Co więcej za każdą błędną odpowiedź uczestnik otrzymuje karę punktową, a poszczególne pytania są różnie
            punktowane. Ocenia się, że aby móc konkurować z najlepszymi zawodnikami, trzeba odpowiedzieć przynajmniej
            70\% pytań z trzema sekundami na odpowiedź na każde z nich. Watson rozegrał wiele gier w ,,Jeopardy'' przeciwko
            najlepszy graczom, a w lutym 2011 roku najlepsi gracze w historii: Ken Jennings i Brad Rutter zostali przez
            niego pokonani. Obecnie system Watson rozwijany jest w stronę systemu ekspertowego i wykorzystywany jest na
            przykład w medycynie. W przeciwieństwie do tradycyjnych systemów ekspertowych działających na podstawie
            reguł IF-THEN, Watson na podstawie dużej ilości danych w języku naturalnym, takich jak  dokumentacja medyczna
            buduje hipotezy dotyczące potencjalnej diagnozy, a następnie przy pomocy algorytmów rankingu ocenia te
            hipotezy\cite{WATSONMEDICINE}.

            Dziedzina odpowiadania na pytania nieustannie się rozwija. Aktualnie badania koncentrują się głównie na:
            \begin{itemize}
                \item prowadzeniu interaktywnego dialogu z użytkownikiem w celu doprecyzowania pytań bądź odpowiwdzi,
                \item wykorzystaniu zasobów lingwistycznych takich jak WordNet i Słowosieć,
                \item respektowanie ograniczeń narzuconych przez pytanie (np. ograniczenia czasowe: Kto był prezydentem
                    Polski w 2013 roku?)
                \item odpowiadanie na pytania z referencjami (np. Ile wynosi bezrobocie wśród kobiet w krajach \emph{europejskich})
            \end{itemize}

        \subsection{Zastosowanie}
            Systemy odpowiadające na pytania pojawiają się wokół nas. Wydaje się, że w przyszłości systemy tego typu będą
            podstawowym sposobem na wyszukiwanie potrzebnych informacji przez użytkowników.

            Jako łatwiejsze w wykorzystaniu i dostarczające bardziej precyzyjnej informacji, systemy QA zastępują tradycyjne
            wyszukiwarki internetowe. Jednym z najstarszych ogólnodostępnych systemów odpowiadających na pytania jest
            system Start stworzony przez grupę InfoLab z MIT. System ten potrafi odpowiedzieć na pytania z dowolnej
            dziedziny, dostarczając konkretną odpowiedź zamiast całego dokumentu, który ją zawiera.
            Aby przeciwdziałać temu trendowi  większość wyszukiwarek internetowych takich jak Google, Bing czy Yahoo
            potrafi wykryć, że wpisane przez użytkownika to nie słowa kluczowe, a pytanie w języku naturalnym.
            W przypadku wykrycia pytania, przeglądarki implementują podejście analogiczne do systemów QA, starając się
            wydobyć odpowiedź na podstawie wcześniej przygotowanej bazy faktów (np. KnowledgeGraph w przypadku Google).

            Odpowiadanie na pytania często zazębia się z chatbotami. W tej formie znajduje zastosowanie w dziedzinie
            aplikacji służących jako inteligentny
            osobisty asystent. Oprogramowanie tego typu stało się popularne i obecnie znajduje się w każdym nowoczesnym
            telefonie komórkowym. Potrafi ono wykonywać dla użytkownika zadania na podstawie poleceń wydawanych w języku
            naturalnym, poprzez rozmowę, podobnie jak w przypadku ludzkiego asystenta. Techniki QA są tutaj głównie
            wykorzystywane do odpowiadania na pytania zadawane przez użytkownika w celu uzyskania konkretnej informacji.
            Tradycyjnie aplikacje tego typu występowały w postaci aplikacji mobilnych, na przykład Siri (iPhone) lub
            Google Now (Android). Niedawno powstały urządzenia typu ,,inteligentny głośnik'' (ang. smart speaker) takie
            jak Amazon Echo oraz Google Home. Po ustawieniu w domu i uruchomieniu, urządzenie oczekuje na polecenia
            zadawane w języku naturalnym.

            Wśród innych zastosowań systemów odpowiadających na pytania można wymienić: wspomaganie wspólnego
            uczenia\cite{COLLABORATIVELEARNING}. Dzięki zastosowaniu QA, nie jest wymagane oczekiwanie na obecność nauczyciela
            w celu odpowiedzi na pytania uczniów. Wymienić też można przytoczone przy omawianiu Watsona budowanie systemów
            pozwalających na wyszukiwanie informacji w nieustrukturyzowanych danych takich jak dokumentacja medyczna,
            sprawozdania finansowe i inne.
\chapter{Systemy odpowiadające na pytania}
    \section{Budowa i działanie}
    \section{System ,,Borsuk''}

\clearpage
\addcontentsline{toc}{chapter}{Bibliografia}
\bibliographystyle{plain}
\bibliography{bibliografia}


\end{document}
